Step 3 — Observe & Verify (lean)
What we add

Per-lecture JSON log line (NDJSON) with just the facts.

Tiny analyzer script to compute: coverage, avg tags, agreement rate, backfill rate, top misses.

Sampler to pull a few tricky cases for human review.

No new product logic. No thresholds. Just visibility.

3.1 Minimal telemetry helper (NDJSON)

Create telemetry.py:

# telemetry.py
import json, os, threading, time
_lock = threading.Lock()

LOG_PATH = os.getenv("TAGGER_LOG_PATH", "/var/log/tagger.ndjson")

def log_line(event: dict):
    event = dict(event)
    event["ts"] = int(time.time() * 1000)
    line = json.dumps(event, ensure_ascii=False)
    with _lock:
        with open(LOG_PATH, "a", encoding="utf-8") as f:
            f.write(line + "\n")

3.2 Emit logs from Step 1 (per-category reasoning)

In category_reasoning.py, after each category returns:

from telemetry import log_line

# inside your per-category loop, right after parse_category_json(...)
log_line({
    "kind": "category_reasoning_result",
    "lecture_id": lecture.get("id") or lecture.get("lecture_id"),
    "category": category,
    "model": model,
    "chosen_ids": out[category].chosen_ids,
    "confidence": out[category].confidence,      # dict
    "rationales_count": len(out[category].rationales or {}),
    # optionally include token/latency if you have them:
    "prompt_tokens": locals().get("prompt_tokens"),
    "latency_ms": locals().get("latency_ms"),
})


(If you don’t track tokens/latency yet, omit those fields.)

3.3 Emit logs from Step 2 (combine + backfill)

In ensemble_scorer.py, after combined is computed:

from telemetry import log_line

# after you build "combined"
log_line({
    "kind": "ensemble_result",
    "lecture_id": lecture_id,  # pass it into the combiner or include in each item
    "backfill_triggered": (len(reasoning_suggestions) == 0 and len(prototype_suggestions) == 0),
    "agreement_count": sum(1 for it in combined if it.get("agreement_bonus_applied")),
    "top3": [
        { "label_id": it["label_id"],
          "combined": it["combined_score"],
          "r": it["reasoning_score"],
          "p": it["prototype_score"],
          "pth": it["prototype_threshold"] }
        for it in combined[:3]
    ],
    "total_returned": len(combined)
})


Also, when you call the combiner, keep the per-item origin:

# when creating reasoning_suggestions in Step 1
reasoning_suggestions.append({
    "label_id": tid,
    "category": cat,
    "confidence": res.confidence.get(tid, 0.0),
    "rationale": res.rationales.get(tid, ""),
    "reasons": ["per_category_reasoning"]
})

3.4 Tiny analyzer (runs offline or via cron)

Create analyze_logs.py:

# analyze_logs.py
import json, sys, statistics as stats
from collections import Counter, defaultdict

def load_lines(path):
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    yield json.loads(line)
                except Exception:
                    pass

def main(path):
    per_lecture = defaultdict(lambda: {"cats": defaultdict(list), "final": None})
    backfill_count = 0
    agreements = []
    totals = []

    for ev in load_lines(path):
        k = ev.get("kind")
        if k == "category_reasoning_result":
            lid = ev.get("lecture_id")
            cat = ev.get("category")
            per_lecture[lid]["cats"][cat] = ev.get("chosen_ids", [])
        elif k == "ensemble_result":
            lid = ev.get("lecture_id")
            per_lecture[lid]["final"] = ev
            backfill_count += 1 if ev.get("backfill_triggered") else 0
            agreements.append(ev.get("agreement_count", 0))
            totals.append(ev.get("total_returned", 0))

    # coverage: lecture has at least one final tag
    lectures = list(per_lecture.keys())
    covered = sum(1 for lid in lectures if (per_lecture[lid]["final"] and per_lecture[lid]["final"].get("total_returned",0) > 0))
    coverage = covered / max(1, len(lectures))

    # per-category non-empty rate (from reasoning step)
    cat_non_empty = Counter()
    cat_total = Counter()
    for lid, data in per_lecture.items():
        for cat, ids in data["cats"].items():
            cat_total[cat] += 1
            if ids:
                cat_non_empty[cat] += 1

    print("=== Summary ===")
    print(f"lectures: {len(lectures)}")
    print(f"coverage (final has ≥1): {coverage:.2%}")
    print(f"backfill_rate: {backfill_count/max(1,len(lectures)):.2%}")
    if totals:
        print(f"avg_final_tags: {stats.mean(totals):.2f} (p50={stats.median(totals):.0f}, max={max(totals)})")
    if agreements:
        print(f"avg_agreements_per_lecture: {stats.mean(agreements):.2f}")

    print("\nper-category non-empty from reasoning:")
    for cat in sorted(cat_total.keys()):
        rate = cat_non_empty[cat] / max(1, cat_total[cat])
        print(f"- {cat}: {rate:.2%} ({cat_non_empty[cat]}/{cat_total[cat]})")

    # flag likely misses: no Topic chosen by reasoning, but prototype had high score in top3
    print("\npossible_misses:")
    for lid, data in per_lecture.items():
        cats = data["cats"]
        final = data["final"] or {}
        top3 = final.get("top3") or []
        has_topic_reasoning = bool(cats.get("Topic"))
        if (not has_topic_reasoning) and any(t.get("p",0) >= (t.get("pth",1.0)) for t in top3):
            print(f"- {lid}: topic empty in reasoning, prototype suggested strong tag (see top3)")

if __name__ == "__main__":
    main(sys.argv[1] if len(sys.argv) > 1 else "/var/log/tagger.ndjson")


Run:

python analyze_logs.py /var/log/tagger.ndjson


You’ll get:

overall coverage

backfill rate

avg final tags

avg agreements

per-category non-empty rate (from reasoning)

a short list of possible misses to sample manually

3.5 Quick sampler for human review (optional)
# sample_human_review.py
import json, random

def sample_cases(path, n=20):
    with open(path, "r", encoding="utf-8") as f:
        lines = [json.loads(l) for l in f if '"ensemble_result"' in l]
    picks = random.sample(lines, min(n, len(lines)))
    for ev in picks:
        lid = ev.get("lecture_id")
        print(f"\n--- {lid} ---")
        for t in ev.get("top3", []):
            print(f"{t['label_id']}: combined={t['combined']:.2f} r={t['r']:.2f} p={t['p']:.2f} (pth={t['pth']:.2f})")

if __name__ == "__main__":
    sample_cases("/var/log/tagger.ndjson", 15)
